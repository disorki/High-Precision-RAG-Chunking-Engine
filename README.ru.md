# High-Precision RAG Chunking Engine & Flowise AI Agent

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)
[![Python: 3.10+](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.100+-green.svg)](https://fastapi.tiangolo.com/)
[![Next.js](https://img.shields.io/badge/Next.js-14-black.svg)](https://nextjs.org/)
[![Docker](https://img.shields.io/badge/Docker-Enabled-blue.svg)](https://www.docker.com/)

> *Read this documentation in [English](README.md)*

Современная высокоточная система поиска и анализа документов на базе архитектуры **RAG (Retrieval-Augmented Generation)**. Проект включает интегрированного ИИ-агента **Flowise (ReAct)** и обеспечивает абсолютную конфиденциальность благодаря **полностью локальной обработке данных** через **Ollama** (флагманская модель Qwen 2.5).

Система спроектирована для глубокого аналитического извлечения фактов, синтеза информации и работы со сложными форматами: текстами, таблицами и отсканированными документами.

---

## Ключевые возможности

*   **Автономный ИИ-Агент (Интеграция Flowise)**
    *   Встроенная архитектура агента ReAct (Reasoning and Acting).
    *   Динамический, самостоятельный выбор инструментов для решения сложных многошаговых запросов пользователя.
*   **Продвинутый RAG-инструментарий (7 специализированных модулей)**
    1.  `search_all`: Глобальный семантический векторный поиск по всей базе знаний.
    2.  `search_document`: Точечный контекстный поиск внутри конкретного документа.
    3.  `list_documents`: Моментальное получение списка проиндексированных файлов.
    4.  `get_document_info`: Глубокий анализ метаданных и структуры файла.
    5.  `summarize`: ИИ-суммаризация и абстрагирование массивных текстовых корпусов.
    6.  `compare_documents`: Кросс-документный фактчекинг и сравнительный анализ.
    7.  `extract_facts`: Прецизионное извлечение конкретных точек данных и структурирование ответов.
*   **Бесшовная облачная интеграция**
    *   Нативная интеграция с Яндекс.Диском через защищенный протокол OAuth 2.0.
    *   Автоматическая синхронизация и пакетный импорт файлов и директорий напрямую из облака.
*   **Интеллектуальный пайплайн обработки**
    *   Нативная поддержка: `PDF` (включая полнотекстовое OCR распознавание сканов через Tesseract/EasyOCR), `DOCX`, `XLSX` и `TXT`.
    *   Автоматическая распаковка и индексирование архивов (`ZIP`, `RAR`) с поддержкой пакетной загрузки.
*   **Премиальный UX/UI интерфейс**
    *   Построен на базе Next.js 14 и React.
    *   Современная эстетика: Flat Design, Glassmorphism, Dark Mode и иконография Lucide.
*   **Локальная среда Zero-Trust**
    *   Стопроцентное on-premise выполнение.
    *   Все данные, документы и LLM-инференс (через Ollama) остаются на вашей машине. Исключены любые утечки во внешние API (OpenAI, Anthropic и т.д.).

---

## Архитектура системы

Проект разделен на 4 отказоустойчивых микросервиса, оркестрируемых через Docker:

1.  **Frontend Модуль (`:3000`)**
    *   *Стек:* Next.js App Router, React, Tailwind CSS.
    *   *Роль:* Предоставляет интерактивный интерфейс загрузки, панель управления Яндекс.Диском и встроенный клиент чата с Агентом.
2.  **Backend Ядро (`:8000`)**
    *   *Стек:* FastAPI (Python), LangChain.
    *   *Роль:* Управляет пайплайном загрузки — парсинг, продвинутый чанкинг (Semantic + Sliding Window), генерация эмбеддингов, векторная маршрутизация (pgvector) и предоставление REST API адаптеров для инструментов Flowise.
3.  **Flowise Оркестратор (`:3001`)**
    *   *Роль:* Визуальный конструктор LLM-пайплайнов, выступающий когнитивным движком ReAct-агента; сохраняет контекст беседы (BufferMemory).
4.  **СУБД и Слой Инференса**
    *   `PostgreSQL + pgvector` (`:5432`): Персистентное хранилище чанков и их векторных представлений.
    *   `Ollama` (`:11434`): Локальный движок инференса, хостящий `qwen2.5:7b` для процессов генерации текста и эмбеддингов.

---

## Руководство по развертыванию

### Требования к среде
*   [Docker Engine](https://docs.docker.com/get-docker/) и Docker Compose.
*   **ОЗУ:** Минимум 16 ГБ (Крайне рекомендуется 32 ГБ для стабильной работы локальных LLM).
*   **GPU:** NVIDIA GPU с установленным [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html) (Настоятельно рекомендуется для аппаратного ускорения Ollama).

### 1. Инициализация контейнеров

Склонируйте репозиторий и запустите сборку Docker окружения:
```bash
git clone https://github.com/your-username/high-precision-rag.git
cd high-precision-rag
docker-compose up --build -d
```

### 2. Подготовка локальной LLM (Ollama)

Дождитесь инициализации сервисов и загрузите целевую модель в локальный инстанс Ollama:
```bash
docker exec -it rag-ollama ollama pull qwen2.5:7b
```
*(Примечание: Вы можете загрузить другие модели эмбеддингов, например `mxbai-embed-large`, обновив конфигурацию Backend).*

### 3. Конфигурация Flowise Агента

1. Откройте панель Flowise по адресу `http://localhost:3001` (Учетные данные: логин `admin`, пароль `admin123`).
2. Перейдите в раздел **Chatflows** и нажмите **Add New**.
3. Импортируйте преднастроенную когнитивную архитектуру: нажмите **Load Chatflow** и выберите файл `flowise_chatflow.json` (в корне репозитория).
4. В настройках данного Chatflow сгенерируйте и скопируйте `API_KEY` и `Chatflow ID`.
5. Внесите эти данные в файл `docker-compose.yml` в список переменных окружения сервиса `frontend`:
   ```yaml
   - NEXT_PUBLIC_FLOWISE_CHATFLOW_ID=ваш_chatflow_id
   - FLOWISE_API_KEY=ваш_api_key
   ```
6. Примените конфигурацию, перезапустив клиентский сервис: `docker restart rag-frontend`

### 4. Использование платформы
Перейдите по адресу **[http://localhost:3000](http://localhost:3000)** для доступа к рабочему пространству.
*   **Вкладка "Обзор":** Загружайте файлы локально или авторизуйте Яндекс.Диск для автоматического импорта.
*   **Вкладки анализа ("Документ", "Все документы"):** Начните интеллектуальное взаимодействие с документами через чат с Агентом.

---

## Локальная разработка

Инструкция для запуска сервисов вне Docker (для дебага и контрибьюции):

**Backend Сервис:**
```bash
cd backend
python -m venv venv
source venv/bin/activate  # Для Windows: venv\Scripts\activate
pip install -r requirements.txt
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

**Frontend Клиент:**
```bash
cd frontend
npm install
npm run dev
```

---

## Лицензия

Проект распространяется под лицензией **MIT License**.
